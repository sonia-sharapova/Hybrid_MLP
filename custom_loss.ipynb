{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a95ce1-a2c3-4f47-903d-5442f51c112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy.sparse as sp\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07dbbda-5ae0-4954-9cf1-bed140f11cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13f157030>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31e2bae-8ccc-4d0a-bf00-ea8a65a53a08",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8675d207-113b-4b3c-bcc0-6b4656f0e894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       A1BG     A2M   A2ML1    AAAS  AACS   AAED1   AAGAB    AAK1   AAMDC  \\\n",
      "0    0.7618  0.7481  0.0000  1.3759   0.0  0.0000  0.0000  0.0160  0.0000   \n",
      "1    1.4195  0.9259  0.0000  0.6716   0.0  0.0000  0.0808  0.1219  0.0000   \n",
      "2    0.3180  0.0000  0.0000  0.1638   0.0  0.0000  0.4738  0.0433  0.0000   \n",
      "3    0.0128  0.0000  0.0000  0.3808   0.0  0.3719  0.0000  0.0169  0.0000   \n",
      "4    0.0000  0.1080  0.0000  0.0000   0.0  0.0000  0.1073  0.0000  0.0000   \n",
      "..      ...     ...     ...     ...   ...     ...     ...     ...     ...   \n",
      "100  0.0771  0.0000  0.7431  0.2510   0.0  0.0000  0.0000  0.1511  0.0000   \n",
      "101  0.0997  0.0000  0.0000  0.0000   0.0  0.0296  0.1863  0.1723  0.0922   \n",
      "102  0.7527  0.5561  3.3277  0.8935   0.0  0.0000  0.0000  0.0975  0.0000   \n",
      "103  0.0126  0.0000  0.0000  0.0796   0.0  0.0000  0.0000  0.2538  0.0000   \n",
      "104  0.9947  0.4070  0.0000  0.1486   0.0  0.0000  0.0000  0.2440  0.0000   \n",
      "\n",
      "       AAMP  ...  ZSWIM8    ZW10  ZWILCH   ZWINT    ZXDC  ZYG11B     ZYX  \\\n",
      "0    0.0000  ...  0.4656  0.2251  0.7310  0.0000  0.0000  1.1508  0.0000   \n",
      "1    0.2943  ...  0.2386  0.0193  0.0000  0.0000  0.0000  0.6027  0.0000   \n",
      "2    0.3008  ...  0.0000  0.0000  0.2109  0.2311  0.0000  0.2617  0.3484   \n",
      "3    0.0000  ...  0.0927  0.2363  0.7675  0.6338  0.0000  0.1919  0.3602   \n",
      "4    0.4147  ...  0.0000  0.0000  0.0000  0.1598  0.5379  0.0397  0.0925   \n",
      "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "100  0.0000  ...  0.1084  0.0598  0.0921  0.0000  0.1626  0.0000  0.0000   \n",
      "101  0.0727  ...  0.1842  0.0000  0.0000  0.1901  0.1515  0.2357  0.0000   \n",
      "102  0.0000  ...  0.4146  0.0000  0.9516  0.6781  0.0000  1.3542  0.0000   \n",
      "103  0.0994  ...  0.0000  0.0000  0.3311  0.0874  0.0000  0.0000  0.0000   \n",
      "104  0.0000  ...  0.0000  0.0000  0.0000  0.0000  0.0000  0.3986  0.0251   \n",
      "\n",
      "      ZZEF1    ZZZ3  cancer_type  \n",
      "0    0.0132  1.6853            1  \n",
      "1    0.0000  0.9144            3  \n",
      "2    0.0000  0.0087            2  \n",
      "3    0.0000  0.3334            1  \n",
      "4    0.0000  0.1183            4  \n",
      "..      ...     ...          ...  \n",
      "100  0.0586  0.2732            1  \n",
      "101  0.0000  0.0072            4  \n",
      "102  0.0000  0.0000            1  \n",
      "103  0.0628  0.4018            4  \n",
      "104  0.0000  0.2670            3  \n",
      "\n",
      "[105 rows x 9734 columns]\n"
     ]
    }
   ],
   "source": [
    "pd_celldata = pd.read_csv(\"proteinOutFileCat_nn.csv\", sep=',', header=0)\n",
    "#pd_celldata = pd.read_csv(\"rnaSeqOutFileCat_nn.csv\", sep=',', header=0)\n",
    "pd_celldata = pd_celldata.rename({'Cancer Type': 'cancer_type'}, axis=1)\n",
    "\n",
    "pd_celldata_t = pd_celldata.drop(columns=\"Composite.Element.REF\")\n",
    "#pd_celldata_t = pd_celldata.drop(columns=\"Hugo_Symbol\")\n",
    "\n",
    "celldata_t = pd_celldata.to_numpy()\n",
    "features = list(pd_celldata)\n",
    "\n",
    "numlist=[]\n",
    "labels = []\n",
    "for ind in range(celldata_t.shape[0]):\n",
    "    x = celldata_t[ind,:]\n",
    "    numlist.append(x)\n",
    "    labels.append(x[-1])\n",
    "\n",
    "numlist = np.array(numlist)\n",
    "print(pd_celldata_t)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = pd_celldata_t.drop(labels='cancer_type', axis=1),pd_celldata_t['cancer_type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3,\n",
    "    stratify=y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "807d1607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AAED1</th>\n",
       "      <th>AAGAB</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>AAMDC</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>...</th>\n",
       "      <th>ZSCAN31</th>\n",
       "      <th>ZSWIM8</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWILCH</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3518</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1941</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3550</td>\n",
       "      <td>0.1677</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4830</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.1502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.9872</td>\n",
       "      <td>0.5819</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>0.1486</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0953</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1261</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1.0195</td>\n",
       "      <td>0.9282</td>\n",
       "      <td>0.5413</td>\n",
       "      <td>0.2318</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2896</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.4174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.8931</td>\n",
       "      <td>1.1238</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.5166</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6783</td>\n",
       "      <td>0.2460</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.2297</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8190</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.8184</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5537</td>\n",
       "      <td>0.1501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.4171</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.1707</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.3472</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2163</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>0.3112</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.5689</td>\n",
       "      <td>0.2099</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2226</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>0.2298</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.5152</td>\n",
       "      <td>0.2688</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0654</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.3093</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1491</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.1467</td>\n",
       "      <td>0.3842</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2608</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1126</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5463</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.1794</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 9733 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A1BG     A2M   A2ML1    AAAS    AACS   AAED1   AAGAB    AAK1   AAMDC  \\\n",
       "40  0.0000  0.3518  0.8609  0.0000  0.0351  0.0000  0.0000  0.3024  0.0000   \n",
       "89  0.9872  0.5819  0.0000  0.0060  0.1486  0.0000  0.0000  0.1880  0.0000   \n",
       "59  1.0195  0.9282  0.5413  0.2318  0.0000  0.0000  0.0000  0.2460  0.0000   \n",
       "74  1.8931  1.1238  0.0297  0.5166  0.0649  0.0000  0.6783  0.2460  0.0000   \n",
       "61  0.2572  0.2297  0.0000  0.0000  0.8190  0.0000  0.8184  0.0000  0.5537   \n",
       "..     ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "91  0.0000  0.0000  1.3472  0.0604  0.0607  0.0000  0.0000  0.2163  0.0000   \n",
       "79  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.2226  0.0000   \n",
       "62  0.5152  0.2688  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000   \n",
       "97  0.0000  0.0000  0.0000  0.0654  0.2572  0.3093  0.0000  0.0000  0.0000   \n",
       "49  0.1467  0.3842  0.0000  0.2608  0.0000  0.0000  0.0000  0.0000  0.0000   \n",
       "\n",
       "      AAMP  ...  ZSCAN31  ZSWIM8    ZW10  ZWILCH   ZWINT    ZXDC  ZYG11B  \\\n",
       "40  0.0005  ...   0.0000  0.1941  0.0000  0.0000  0.3550  0.1677  0.0000   \n",
       "89  0.0604  ...   0.0000  0.0000  0.0953  0.0000  0.1261  0.0000  0.0000   \n",
       "59  0.0000  ...   0.0000  0.0000  0.0000  0.0059  0.0000  0.4014  0.0000   \n",
       "74  0.0000  ...   0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000   \n",
       "61  0.1501  ...   0.0000  0.4171  0.0877  0.5070  0.1707  0.0000  0.0000   \n",
       "..     ...  ...      ...     ...     ...     ...     ...     ...     ...   \n",
       "91  0.0000  ...   0.0000  0.0228  0.3112  0.2540  0.0606  0.5689  0.2099   \n",
       "79  0.2458  ...   0.0000  0.0215  0.0000  0.1579  0.1984  0.0000  0.0809   \n",
       "62  0.1863  ...   0.0000  0.0347  0.0000  0.0000  0.0000  0.0000  0.2333   \n",
       "97  0.3358  ...   0.2045  0.0000  0.1491  0.0000  0.0000  0.0000  0.0000   \n",
       "49  0.1860  ...   0.0000  0.0000  0.0000  0.1126  0.0000  0.0727  0.0000   \n",
       "\n",
       "       ZYX   ZZEF1    ZZZ3  \n",
       "40  0.4830  0.0311  0.1502  \n",
       "89  0.0439  0.1576  0.0000  \n",
       "59  0.2896  0.0741  0.4174  \n",
       "74  0.0000  0.0000  0.5529  \n",
       "61  0.0000  0.2301  0.0000  \n",
       "..     ...     ...     ...  \n",
       "91  0.0000  0.1453  0.0000  \n",
       "79  0.2298  0.0000  0.1296  \n",
       "62  0.0000  0.0000  0.2594  \n",
       "97  0.2782  0.0000  0.0000  \n",
       "49  0.5463  0.0309  0.1794  \n",
       "\n",
       "[73 rows x 9733 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71daed21",
   "metadata": {},
   "source": [
    "## Baseline Gene Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82c43544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n",
      "Mean Squared Error: 0.56\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([5478, 2164, 4438, ..., 6279, 6280,    1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # for classification tasks\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def randomForestBaseline(X_train, y_train, X_test, y_test):\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust hyperparameters like n_estimators\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = clf.feature_importances_\n",
    "\n",
    "    # Sort the feature importances and create a ranking (add one to start at index 1)\n",
    "    feature_ranking = np.argsort(feature_importances)[::-1] + 1\n",
    "    \n",
    "    feature_names = list(pd_celldata_t.columns)  # Replace with your actual feature names or column labels\n",
    "\n",
    "    # Create a list of (importance, feature_name) tuples\n",
    "    feature_importance_tuples = list(zip(feature_ranking, feature_names))\n",
    "        \n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Mean Squared Error: {mse:.2f}')\n",
    "    \n",
    "    \n",
    "    return feature_importance_tuples, feature_ranking\n",
    "\n",
    "feature_importance_tuples, feature_ranking = randomForestBaseline(X_train, y_train, X_test, y_test)\n",
    "feature_ranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd201ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d43b6902-fa66-45bc-b5c0-5fb7ebac6bc3",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "645b62e9-9747-422c-a8eb-8253fcacc470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanpy==1.9.6 anndata==0.10.3 umap==0.5.4 numpy==1.24.3 scipy==1.11.1 pandas==2.0.3 scikit-learn==1.3.0 statsmodels==0.14.0 pynndescent==0.5.10\n"
     ]
    }
   ],
   "source": [
    "X_train_norm1 = X_train.div(X_train.sum(axis=1), axis=0)\n",
    "X_train_norm2 = X_train.div(X_train.sum(axis=0), axis=1)\n",
    "X_train_norm2=X_train_norm2.fillna(0)\n",
    "\n",
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.logging.print_header()\n",
    "sc.settings.set_figure_params(dpi=80, facecolor=\"white\")\n",
    "\n",
    "X_train_a = np.array(X_train_norm1)\n",
    "adata = ad.AnnData(X_train_a)\n",
    "#adata.uns[\"name\"] = \"rna_seq\"\n",
    "adata.uns[\"name\"] = \"prot\"\n",
    "\n",
    "y_train_str = []\n",
    "for i in y_train:\n",
    "    y_train_str.append(str(i)) # convert to strings so that they can be recognized by scanpy\n",
    "adata.obs['true_labels'] = y_train_str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e1b502-bfb5-4099-8811-c6c18ba026a3",
   "metadata": {},
   "source": [
    "## preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce26650d-f8a3-4ef8-a62f-0c11fb9fd8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_labels\n",
       "0            1\n",
       "1            3\n",
       "2            1\n",
       "3            3\n",
       "4            3\n",
       "..         ...\n",
       "68           3\n",
       "69           0\n",
       "70           4\n",
       "71           4\n",
       "72           3\n",
       "\n",
       "[73 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef03d9de-6476-45bc-9f52-361d344d1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X_train.iloc[idx].values  # Extract values from X_train DataFrame row\n",
    "        y = self.y_train.iloc[idx]  # Extract label from y_train DataFrame\n",
    "        return torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.long)  # Convert to PyTorch tensors\n",
    "\n",
    "def preprocess(adata):\n",
    "    sc.pp.filter_genes(adata, min_cells=1)\n",
    "    sc.pp.normalize_total(adata)\n",
    "    sc.pp.neighbors(adata, use_rep='X')\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca459746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an Autoencoder (AE) model \n",
    "\n",
    "##### Original ####\n",
    "\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4*1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*1024, 4*512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*512, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4*512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*512, 4*1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*1024, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        latent_representation = self.encoder(x)\n",
    "        recon_batch = self.decoder(latent_representation)\n",
    "        return recon_batch, latent_representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771e49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f8a7607-65e4-4d1b-b0b8-8f5d072447d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a Multi-Layer Perceptron (MLP) model \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_prob=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # Apply dropout\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# Combine AE and MLP models\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, ae, mlp):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.ae = ae\n",
    "        self.mlp = mlp\n",
    "    \n",
    "    def forward(self, x):\n",
    "        recon_batch, latent_representation = self.ae(x)\n",
    "        mlp_output = self.mlp(latent_representation)\n",
    "        return recon_batch, latent_representation, mlp_output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77d5e9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass CustomMSELossWithSparsity(nn.Module):\\n    def __init__(self, sparsity_weight=0.01):\\n        super(CustomMSELossWithSparsity, self).__init__()\\n        self.sparsity_weight = sparsity_weight\\n\\n    def forward(self, input, target, latent_representation):\\n        # Calculate the Mean Squared Error\\n        mse = torch.mean((input - target)**2)\\n        \\n        # Calculate the sparsity term (e.g., L1 regularization on latent_representation)\\n        sparsity_term = torch.abs(latent_representation).sum()\\n        \\n        # Combine MSE loss and sparsity term\\n        total_loss = mse + self.sparsity_weight * sparsity_term\\n\\n        return total_loss\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class CustomMSELossWithSparsity(nn.Module):\n",
    "    def __init__(self, sparsity_weight=0.01):\n",
    "        super(CustomMSELossWithSparsity, self).__init__()\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "\n",
    "    def forward(self, input, target, latent_representation):\n",
    "        # Calculate the Mean Squared Error\n",
    "        mse = torch.mean((input - target)**2)\n",
    "        \n",
    "        # Calculate the sparsity term (e.g., L1 regularization on latent_representation)\n",
    "        sparsity_term = torch.abs(latent_representation).sum()\n",
    "        \n",
    "        # Combine MSE loss and sparsity term\n",
    "        total_loss = mse + self.sparsity_weight * sparsity_term\n",
    "\n",
    "        return total_loss\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5110a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom loss function with L1 regularization\n",
    "class AutoencoderLoss(nn.Module):\n",
    "    def __init__(self, lambda_l1=0.001):\n",
    "        super(AutoencoderLoss, self).__init__()\n",
    "        self.lambda_l1 = lambda_l1\n",
    "\n",
    "    def forward(self, output, target, encoder):\n",
    "        # Calculate the reconstruction loss (MSE)\n",
    "        reconstruction_loss = nn.MSELoss()(output, target)\n",
    "\n",
    "        # Calculate the L1 regularization loss on the encoder tensor\n",
    "        l1_loss = torch.abs(encoder).sum()\n",
    "\n",
    "        # Combine the reconstruction loss and L1 loss with a weighting factor\n",
    "        ae_loss = reconstruction_loss + self.lambda_l1 * l1_loss\n",
    "\n",
    "        return ae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c60f7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self, rank_weights):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "        self.rank_weights = torch.Tensor(rank_weights.copy())\n",
    "\n",
    "    def forward(self, recon, data):\n",
    "        # Calculate the Mean Squared Error\n",
    "        #mse = torch.mean((input - target)**2)\n",
    "        weighted_errors = torch.square(recon - data) * (1 + (3/self.rank_weights))  # Apply feature weights\n",
    "        weighted_loss = torch.mean(weighted_errors)\n",
    "        \n",
    "        #weighted_loss = torch.sum((1+(1/self.rank_weights)) * torch.square(recon - data))\n",
    "        return weighted_loss\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfaccc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "#input_dim = len(adata.var)\n",
    "input_dim = len(numlist[0]) - 2\n",
    "\n",
    "output_dim = 5\n",
    "#reaches 78.12% with output_dim=10 but stabalizes at 75%\n",
    "batch_size = 757\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Initialize AE and MLP models with increased complexity and dropout\n",
    "ae = AE(input_dim, latent_dim)\n",
    "mlp = MLP(latent_dim, output_dim)\n",
    "\n",
    "# Create the combined model\n",
    "combined_model = CombinedModel(ae, mlp)\n",
    "\n",
    "#autoencoder_criterion = nn.MSELoss()\n",
    "#autoencoder_criterion = CustomMSELossWithSparsity(sparsity_weight=0.01)\n",
    "#autoencoder_criterion = CustomMSELoss()\n",
    "lambda_l1 = 0.01\n",
    "ae_criterion = AutoencoderLoss(lambda_l1)\n",
    "mlp_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_combined = optim.AdamW(combined_model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_combined, step_size=5, gamma=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96b34287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data loaders\n",
    "\n",
    "X_df_train = pd.DataFrame(X_train)\n",
    "y_df_train = pd.Series(y_train)\n",
    "\n",
    "dataset_train = MyDataset(X_df_train, y_df_train)\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "X_df_test = pd.DataFrame(X_test)\n",
    "y_df_test = pd.Series(y_test)\n",
    "\n",
    "dataset_test = MyDataset(X_df_test, y_df_test)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10468319-da59-4ba2-a702-2475e9c9a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def composite_loss(autoencoder_loss, mlp_loss, custom_loss_weight):\n",
    "    return autoencoder_loss + mlp_loss + (custom_loss_weight * custom_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ed723-3ad8-4a2c-828f-a70b27ab68be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af884fcc-3617-47d4-bb56-ff8af0528e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [0/1], AE Loss: 0.2175, MLP Loss: 1.6162\n",
      "Epoch [1/50], Combined Model Accuracy on Validation/Test dataset: 31.25%\n",
      "Epoch [2/50], Batch [0/1], AE Loss: 0.1452, MLP Loss: 1.5138\n",
      "Epoch [2/50], Combined Model Accuracy on Validation/Test dataset: 28.12%\n",
      "Epoch [3/50], Batch [0/1], AE Loss: 0.1408, MLP Loss: 1.6586\n",
      "Epoch [3/50], Combined Model Accuracy on Validation/Test dataset: 31.25%\n",
      "Epoch [4/50], Batch [0/1], AE Loss: 0.1464, MLP Loss: 1.4003\n",
      "Epoch [4/50], Combined Model Accuracy on Validation/Test dataset: 31.25%\n",
      "Epoch [5/50], Batch [0/1], AE Loss: 0.1301, MLP Loss: 1.2075\n",
      "Epoch [5/50], Combined Model Accuracy on Validation/Test dataset: 65.62%\n",
      "Epoch [6/50], Batch [0/1], AE Loss: 0.1205, MLP Loss: 0.9181\n",
      "Epoch [6/50], Combined Model Accuracy on Validation/Test dataset: 62.50%\n",
      "Epoch [7/50], Batch [0/1], AE Loss: 0.1197, MLP Loss: 0.7378\n",
      "Epoch [7/50], Combined Model Accuracy on Validation/Test dataset: 68.75%\n",
      "Epoch [8/50], Batch [0/1], AE Loss: 0.1188, MLP Loss: 0.5298\n",
      "Epoch [8/50], Combined Model Accuracy on Validation/Test dataset: 71.88%\n",
      "Epoch [9/50], Batch [0/1], AE Loss: 0.1170, MLP Loss: 0.3596\n",
      "Epoch [9/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [10/50], Batch [0/1], AE Loss: 0.1201, MLP Loss: 0.2183\n",
      "Epoch [10/50], Combined Model Accuracy on Validation/Test dataset: 68.75%\n",
      "Epoch [11/50], Batch [0/1], AE Loss: 0.1121, MLP Loss: 0.1307\n",
      "Epoch [11/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [12/50], Batch [0/1], AE Loss: 0.1118, MLP Loss: 0.1031\n",
      "Epoch [12/50], Combined Model Accuracy on Validation/Test dataset: 78.12%\n",
      "Epoch [13/50], Batch [0/1], AE Loss: 0.1101, MLP Loss: 0.0843\n",
      "Epoch [13/50], Combined Model Accuracy on Validation/Test dataset: 78.12%\n",
      "Epoch [14/50], Batch [0/1], AE Loss: 0.1062, MLP Loss: 0.0714\n",
      "Epoch [14/50], Combined Model Accuracy on Validation/Test dataset: 78.12%\n",
      "Epoch [15/50], Batch [0/1], AE Loss: 0.1049, MLP Loss: 0.0610\n",
      "Epoch [15/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [16/50], Batch [0/1], AE Loss: 0.1024, MLP Loss: 0.0534\n",
      "Epoch [16/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [17/50], Batch [0/1], AE Loss: 0.1001, MLP Loss: 0.0506\n",
      "Epoch [17/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [18/50], Batch [0/1], AE Loss: 0.0987, MLP Loss: 0.0483\n",
      "Epoch [18/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [19/50], Batch [0/1], AE Loss: 0.0980, MLP Loss: 0.0464\n",
      "Epoch [19/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [20/50], Batch [0/1], AE Loss: 0.0967, MLP Loss: 0.0447\n",
      "Epoch [20/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [21/50], Batch [0/1], AE Loss: 0.0951, MLP Loss: 0.0432\n",
      "Epoch [21/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [22/50], Batch [0/1], AE Loss: 0.0943, MLP Loss: 0.0424\n",
      "Epoch [22/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [23/50], Batch [0/1], AE Loss: 0.0937, MLP Loss: 0.0416\n",
      "Epoch [23/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [24/50], Batch [0/1], AE Loss: 0.0932, MLP Loss: 0.0408\n",
      "Epoch [24/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [25/50], Batch [0/1], AE Loss: 0.0926, MLP Loss: 0.0399\n",
      "Epoch [25/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [26/50], Batch [0/1], AE Loss: 0.0918, MLP Loss: 0.0391\n",
      "Epoch [26/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [27/50], Batch [0/1], AE Loss: 0.0914, MLP Loss: 0.0387\n",
      "Epoch [27/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [28/50], Batch [0/1], AE Loss: 0.0910, MLP Loss: 0.0383\n",
      "Epoch [28/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [29/50], Batch [0/1], AE Loss: 0.0906, MLP Loss: 0.0378\n",
      "Epoch [29/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [30/50], Batch [0/1], AE Loss: 0.0902, MLP Loss: 0.0374\n",
      "Epoch [30/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [31/50], Batch [0/1], AE Loss: 0.0899, MLP Loss: 0.0369\n",
      "Epoch [31/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [32/50], Batch [0/1], AE Loss: 0.0897, MLP Loss: 0.0366\n",
      "Epoch [32/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [33/50], Batch [0/1], AE Loss: 0.0896, MLP Loss: 0.0364\n",
      "Epoch [33/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [34/50], Batch [0/1], AE Loss: 0.0894, MLP Loss: 0.0361\n",
      "Epoch [34/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [35/50], Batch [0/1], AE Loss: 0.0892, MLP Loss: 0.0359\n",
      "Epoch [35/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [36/50], Batch [0/1], AE Loss: 0.0890, MLP Loss: 0.0356\n",
      "Epoch [36/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [37/50], Batch [0/1], AE Loss: 0.0889, MLP Loss: 0.0354\n",
      "Epoch [37/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [38/50], Batch [0/1], AE Loss: 0.0888, MLP Loss: 0.0353\n",
      "Epoch [38/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [39/50], Batch [0/1], AE Loss: 0.0887, MLP Loss: 0.0352\n",
      "Epoch [39/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [40/50], Batch [0/1], AE Loss: 0.0886, MLP Loss: 0.0350\n",
      "Epoch [40/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [41/50], Batch [0/1], AE Loss: 0.0885, MLP Loss: 0.0349\n",
      "Epoch [41/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [42/50], Batch [0/1], AE Loss: 0.0884, MLP Loss: 0.0348\n",
      "Epoch [42/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [43/50], Batch [0/1], AE Loss: 0.0884, MLP Loss: 0.0347\n",
      "Epoch [43/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [44/50], Batch [0/1], AE Loss: 0.0883, MLP Loss: 0.0346\n",
      "Epoch [44/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [45/50], Batch [0/1], AE Loss: 0.0883, MLP Loss: 0.0346\n",
      "Epoch [45/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [46/50], Batch [0/1], AE Loss: 0.0883, MLP Loss: 0.0345\n",
      "Epoch [46/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [47/50], Batch [0/1], AE Loss: 0.0882, MLP Loss: 0.0344\n",
      "Epoch [47/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [48/50], Batch [0/1], AE Loss: 0.0882, MLP Loss: 0.0344\n",
      "Epoch [48/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [49/50], Batch [0/1], AE Loss: 0.0882, MLP Loss: 0.0344\n",
      "Epoch [49/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n",
      "Epoch [50/50], Batch [0/1], AE Loss: 0.0882, MLP Loss: 0.0343\n",
      "Epoch [50/50], Combined Model Accuracy on Validation/Test dataset: 75.00%\n"
     ]
    }
   ],
   "source": [
    "custom_ae_loss = CustomMSELoss(feature_ranking)\n",
    "\n",
    "num_epochs = 50\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (X_train, y_train) in enumerate(train_loader):\n",
    "        X_train = X_train.view(X_train.size(0), -1)\n",
    "                               \n",
    "        #optimizer_combined.zero_grad()\n",
    "\n",
    "        recon_batch, latent_representation, mlp_output_train = combined_model(X_train)\n",
    "        #print(latent_representation)\n",
    "        #recon_batch, mlp_output_train = combined_model(X_train)\n",
    "\n",
    "        # Calculate AE loss (reconstruction loss)\n",
    "        #ae_loss = CustomMSELossWithSparsity(recon_batch, data)\n",
    "    \n",
    "        mlp_loss = mlp_criterion(mlp_output_train, y_train)\n",
    "        #ae_loss = ae_criterion(recon_batch, X_train, latent_representation)\n",
    "        ae_loss = custom_ae_loss(recon_batch, X_train)\n",
    "        #ae_loss = nn.MSELoss()(recon_batch, X_train)\n",
    "        \n",
    "        # Calculate MLP loss \n",
    "        #mlp_loss = nn.CrossEntropyLoss()(mlp_output_train, y_train)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Total loss\n",
    "        total_loss = ae_loss + mlp_loss\n",
    "\n",
    "        optimizer_combined.zero_grad()\n",
    "        total_loss.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "        optimizer_combined.step()\n",
    "\n",
    "        #ordered.grad = torch.autograd.grad(ae_loss, custom_loss.gene_ranking)[0]\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], AE Loss: {ae_loss.item():.4f}, MLP Loss: {mlp_loss.item():.4f}')\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Evaluate accuracy on validation/test dataset\n",
    "    combined_model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:  \n",
    "            X_test = X_test.view(X_test.size(0), -1)\n",
    "            _, _, mlp_output = combined_model(X_test)\n",
    "            _, predicted = torch.max(mlp_output, 1)\n",
    "            total += y_test.size(0)\n",
    "            correct += (predicted == y_test).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Combined Model Accuracy on Validation/Test dataset: {100 * accuracy:.2f}%')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96218948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d00a2e7-0a96-4a08-8fc9-1e47da717827",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convergence criteria\n",
    "max_iterations = 10  # Adjust as needed\n",
    "convergence_threshold = 0.001  # Adjust as needed\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    # Train the model with the current gene rankings\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data = data.view(data.size(0), -1)\n",
    "            # Forward pass, compute loss, and backpropagate\n",
    "            # Use the custom loss function with the current gene rankings\n",
    "            optimizer_combined.zero_grad()\n",
    "            gene_rankings = ordered.clone().detach().requires_grad_(True)\n",
    "            recon_batch, mlp_output = combined_model(data)\n",
    "            print(recon_batch)\n",
    "            print(mlp_output)\n",
    "            print(labels)\n",
    "            loss = CustomLoss(ordered)(latent_representation, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Check for convergence by comparing updated gene rankings\n",
    "        new_gene_rankings = ordered.detach().numpy()\n",
    "        if torch.norm(new_gene_rankings - ordered) < convergence_threshold:\n",
    "            break  # Converged\n",
    "\n",
    "        # Update gene rankings based on the current model (you can define your update logic)\n",
    "        ordered = new_gene_rankings\n",
    "\n",
    "# Final gene rankings after convergence\n",
    "final_gene_rankings = new_gene_rankings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4101957-8a61-46c3-ab28-109aab99edb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdaffd-7881-4d2d-b4b6-e631c499d3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0243ac54-c063-4ff5-97ca-93f1802cbe49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4c7f0-5540-4f65-a3e3-71157d53c9b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a2c38-62c0-4637-8357-284f836cc051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a7ed8-3d97-4a95-8f97-3178fffeaf68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa11a55-10bb-4b3d-b50c-37586dc5d2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85db08cb-e347-48a4-aa6a-67c838a65a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45142d33-07b9-47c2-99fc-5febb4dac542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf499d-551d-4087-98d5-8b99c1bd5167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ad93b-c9fb-4723-937c-2b37d2e0f301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1479225-77bb-4a0d-b2d8-581c7e094650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc9c82b-9767-4165-a00b-bece705e2b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e07fc2-0ef6-4500-901e-866335c2a2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
